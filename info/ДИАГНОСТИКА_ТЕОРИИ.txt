═══════════════════════════════════════════════════════════════════════════
ДИАГНОСТИКА ПРОБЛЕМ С ГЕНЕРАЦИЕЙ ТЕОРИИ
═══════════════════════════════════════════════════════════════════════════

Если генерация теоретических объяснений не работает, следуйте этой инструкции.

═══════════════════════════════════════════════════════════════════════════

ШАГ 1: ПРОВЕРКА OLLAMA
───────────────────────────────────────────────────────────────────────────

1. Убедитесь, что Ollama установлен:

    ollama --version

   Если команда не найдена, установите Ollama:
   https://ollama.ai/download

2. Запустите Ollama сервер:

    ollama serve

   Должно появиться сообщение о запуске на порту 11434.
   НЕ закрывайте это окно терминала!

3. В ДРУГОМ терминале проверьте, что модель загружена:

    ollama list

   Если deepseek-r1:7b нет в списке, скачайте её:

    ollama pull deepseek-r1:7b

   Это займёт ~5GB и несколько минут.

4. Проверьте работу модели:

    ollama run deepseek-r1:7b

   Введите: "Привет, как дела?"
   Модель должна ответить на русском языке.
   Нажмите Ctrl+D для выхода.

═══════════════════════════════════════════════════════════════════════════

ШАГ 2: ПРОВЕРКА ПОДКЛЮЧЕНИЯ К LLM
───────────────────────────────────────────────────────────────────────────

Запустите тестовый скрипт:

    python test_theory_generation.py

Скрипт проверит:
✓ Доступность LLM клиента
✓ Генерацию теории для 10 различных предметов
✓ Работу кэширования

Ожидаемое время: 3-10 минут (зависит от скорости ПК).

═══════════════════════════════════════════════════════════════════════════

ШАГ 3: АНАЛИЗ ЛОГОВ
───────────────────────────────────────────────────────────────────────────

При генерации теории система выводит детальные логи:

[INFO] Генерация объяснения через LLM: Физика/Механика/Кинематика
[INFO] Создание промпта для темы: Кинематика
[INFO] Контекст: стиль=научный, фокус=законы физики
[INFO] Отправка запроса к LLM...
[INFO] Получен ответ от LLM (длина: 1543)
[INFO] Очистка ответа от служебных тегов...
[INFO] Сохранение в кэш...
[INFO] Объяснение сохранено в кэш: bot/explanations/kinematika.txt
[SUCCESS] Объяснение успешно сгенерировано и сохранено

Если видите ошибки - переходите к Шагу 4.

═══════════════════════════════════════════════════════════════════════════

ШАГ 4: ТИПИЧНЫЕ ОШИБКИ И РЕШЕНИЯ
───────────────────────────────────────────────────────────────────────────

❌ [ERROR] LLM клиент не инициализирован
   ✓ Решение: Запустите Ollama: ollama serve
   ✓ Проверьте порт: netstat -an | grep 11434 (должен быть LISTENING)

❌ [ERROR] Ошибка при обращении к LLM: Connection refused
   ✓ Решение: Ollama не запущен. Запустите: ollama serve
   ✓ Если порт 11434 занят, завершите процесс: pkill ollama

❌ [ERROR] Model not found: deepseek-r1:7b
   ✓ Решение: Скачайте модель: ollama pull deepseek-r1:7b
   ✓ Проверьте список моделей: ollama list

❌ [WARN] Объяснение слишком короткое: 12 символов
   ✓ Решение: LLM вернул некорректный ответ
   ✓ Попробуйте ещё раз (regenerate=True)
   ✓ Проверьте логи Ollama на ошибки

❌ [ERROR] LLM вернул пустой ответ
   ✓ Решение: Проблема с Ollama или моделью
   ✓ Перезапустите Ollama: pkill ollama && ollama serve
   ✓ Переустановите модель: ollama rm deepseek-r1:7b && ollama pull deepseek-r1:7b

❌ [ERROR] Ошибка сохранения в кэш: Permission denied
   ✓ Решение: Нет прав на запись в папку bot/explanations/
   ✓ Создайте папку: mkdir -p bot/explanations
   ✓ Дайте права: chmod 755 bot/explanations

❌ Медленная генерация (>2 минут на тему)
   ✓ Решение: Нормально для слабых ПК
   ✓ Ускорение: используйте GPU (если есть)
   ✓ Альтернатива: используйте более лёгкую модель

═══════════════════════════════════════════════════════════════════════════

ШАГ 5: ПРОВЕРКА СИСТЕМНЫХ ТРЕБОВАНИЙ
───────────────────────────────────────────────────────────────────────────

Минимальные требования для работы deepseek-r1:7b:

CPU: Любой современный процессор (Intel/AMD/ARM)
RAM: 8GB (рекомендуется 16GB)
Диск: 5GB свободного места
Python: 3.8+

Если не хватает ресурсов, рассмотрите альтернативы:

1. Более лёгкая модель (не рекомендуется, хуже качество):
   ollama pull llama2:7b

2. Облачный API (требует интернет):
   Измените bot/chat.py для использования OpenAI API

═══════════════════════════════════════════════════════════════════════════

ШАГ 6: РУЧНАЯ ПРОВЕРКА
───────────────────────────────────────────────────────────────────────────

Проверьте генерацию вручную в Python:

python
>>> from bot import chat
>>> from bot.prompt import Prompt
>>> 
>>> # Проверяем доступность
>>> print(chat.academic.is_available())
True
>>> 
>>> # Создаём простой промпт
>>> prompt = Prompt(
...     role="Ты учитель математики",
...     task="Объясни, что такое линейное уравнение",
...     answer="Дай краткое объяснение"
... )
>>> 
>>> # Отправляем запрос
>>> response = chat.academic.ask(prompt)
>>> print(len(response))
542
>>> 
>>> print(response[:200])
Линейное уравнение — это уравнение, в котором переменная входит 
только в первой степени...

Если это работает, проблема в другом модуле.

═══════════════════════════════════════════════════════════════════════════

ШАГ 7: ПРОВЕРКА КЭШИРОВАНИЯ
───────────────────────────────────────────────────────────────────────────

Система кэширует объяснения в папке bot/explanations/

Проверьте содержимое:

    ls -lh bot/explanations/

Каждая тема сохраняется в отдельный файл с транслитерацией названия:
- "Линейные уравнения" → lineinye_uravneniya.txt
- "Кинематика" → kinematika.txt

Если кэш не работает:

1. Проверьте права:
   ls -ld bot/explanations/

2. Создайте папку вручную:
   mkdir -p bot/explanations

3. Очистите кэш при проблемах:
   rm -rf bot/explanations/*.txt

═══════════════════════════════════════════════════════════════════════════

ШАГ 8: FALLBACK СИСТЕМА
───────────────────────────────────────────────────────────────────────────

Если LLM не работает, система автоматически:

1. Проверяет КЭШИРОВАННЫЕ объяснения
2. Использует ЛОКАЛЬНЫЕ объяснения (предопределённые)
3. Показывает СООБЩЕНИЕ ОБ ОШИБКЕ с инструкциями

Локальные объяснения есть только для:
- Линейные уравнения (Алгебра)

Для других тем fallback не сработает без LLM.

═══════════════════════════════════════════════════════════════════════════

ШАГ 9: ПРОДВИНУТАЯ ДИАГНОСТИКА
───────────────────────────────────────────────────────────────────────────

Проверьте логи Ollama:

1. Запустите Ollama с подробными логами:
   OLLAMA_DEBUG=1 ollama serve

2. Следите за выводом при генерации теории

3. Ищите ошибки типа:
   - "out of memory"
   - "model not loaded"
   - "timeout"

Проверьте использование ресурсов:

    # CPU и RAM
    top | grep ollama
    
    # GPU (если есть)
    nvidia-smi

═══════════════════════════════════════════════════════════════════════════

ШАГ 10: ОБРАЩЕНИЕ ЗА ПОМОЩЬЮ
───────────────────────────────────────────────────────────────────────────

Если ничего не помогло, соберите диагностическую информацию:

1. Версия Ollama:
   ollama --version

2. Список моделей:
   ollama list

3. Логи приложения (сохраните вывод):
   python test_theory_generation.py > theory_test.log 2>&1

4. Системная информация:
   uname -a
   python --version
   pip list | grep -E "langchain|ollama"

5. Проверка подключения:
   curl http://localhost:11434/api/tags

═══════════════════════════════════════════════════════════════════════════

БЫСТРЫЕ КОМАНДЫ ДЛЯ ДИАГНОСТИКИ
───────────────────────────────────────────────────────────────────────────

# Перезапуск всего (решает 90% проблем)
pkill ollama
ollama serve &
sleep 5
python test_theory_generation.py

# Полная переустановка модели
ollama rm deepseek-r1:7b
ollama pull deepseek-r1:7b

# Очистка кэша
rm -rf bot/explanations/*.txt

# Проверка занятых портов
lsof -i :11434

═══════════════════════════════════════════════════════════════════════════

УЛУЧШЕНИЯ В КОДЕ
───────────────────────────────────────────────────────────────────────────

Обновлённый bot/theory.py теперь включает:

✅ Детальное логирование каждого шага
✅ Проверку доступности LLM перед генерацией
✅ Улучшенные сообщения об ошибках
✅ Информацию о длине сгенерированного текста
✅ Логирование операций с кэшем
✅ Вывод traceback при исключениях
✅ Информацию о контексте генерации

Все ошибки теперь видны в консоли для быстрой диагностики.

═══════════════════════════════════════════════════════════════════════════

КОНТАКТЫ И ДОКУМЕНТАЦИЯ
───────────────────────────────────────────────────────────────────────────

Тестовый скрипт: test_theory_generation.py
Код модуля: bot/theory.py
LLM клиент: bot/llm.py
Конфигурация: bot/chat.py

Ollama документация: https://ollama.ai/
DeepSeek-R1 информация: https://github.com/deepseek-ai/DeepSeek-R1

═══════════════════════════════════════════════════════════════════════════

